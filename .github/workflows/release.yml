name: Build and Release

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:

jobs:
  build-macos-metal:
    name: Build macOS (Apple Silicon + Metal)
    runs-on: macos-latest
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Install dependencies
        run: |
          brew install cmake
          
      - name: Build llama.cpp with Metal
        run: |
          make build-llama-metal
          
      - name: Build inference service
        run: |
          make build-all
          
      - name: Test build
        run: |
          ./bin/inference-server --help || true
          ./bin/nats-chat --help || true
          
      - name: Package macOS release
        run: |
          mkdir -p dist/macos-arm64
          cp bin/inference-server dist/macos-arm64/
          cp bin/nats-chat dist/macos-arm64/
          cp -r envs dist/macos-arm64/
          cp -r examples dist/macos-arm64/
          cp -r scripts dist/macos-arm64/
          cp -r data dist/macos-arm64/
          cp Makefile dist/macos-arm64/
          cp README.md dist/macos-arm64/
          cp LICENSE dist/macos-arm64/
          cd dist && zip -r inference-service-macos-arm64-metal.zip macos-arm64/
          
      - name: Upload macOS artifact
        uses: actions/upload-artifact@v4
        with:
          name: macos-arm64-metal
          path: dist/inference-service-macos-arm64-metal.zip

  build-linux-amd64:
    name: Build Linux AMD64 (CPU)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake
          
      - name: Build llama.cpp (CPU)
        run: |
          make build-llama-cpu
          
      - name: Build inference service
        run: |
          make build-all
          
      - name: Test build
        run: |
          ./bin/inference-server --help || true
          ./bin/nats-chat --help || true
          
      - name: Package Linux AMD64 release
        run: |
          mkdir -p dist/linux-amd64
          cp bin/inference-server dist/linux-amd64/
          cp bin/nats-chat dist/linux-amd64/
          cp -r envs dist/linux-amd64/
          cp -r examples dist/linux-amd64/
          cp -r scripts dist/linux-amd64/
          cp -r data dist/linux-amd64/
          cp Makefile dist/linux-amd64/
          cp README.md dist/linux-amd64/
          cp LICENSE dist/linux-amd64/
          cd dist && tar -czf inference-service-linux-amd64-cpu.tar.gz linux-amd64/
          
      - name: Upload Linux AMD64 artifact
        uses: actions/upload-artifact@v4
        with:
          name: linux-amd64-cpu
          path: dist/inference-service-linux-amd64-cpu.tar.gz

  build-linux-cuda:
    name: Build Linux AMD64 (CUDA)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Install CUDA Toolkit
        uses: Jimver/cuda-toolkit@v0.2.24
        with:
          cuda: '12.4'

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake
          
      - name: Build llama.cpp with CUDA
        run: |
          make build-llama-cuda
          
      - name: Build inference service
        run: |
          make build-all
          
      - name: Test build
        run: |
          ./bin/inference-server --help || true
          ./bin/nats-chat --help || true
          
      - name: Package Linux CUDA release
        run: |
          mkdir -p dist/linux-amd64-cuda
          cp bin/inference-server dist/linux-amd64-cuda/
          cp bin/nats-chat dist/linux-amd64-cuda/
          cp -r envs dist/linux-amd64-cuda/
          cp -r examples dist/linux-amd64-cuda/
          cp -r scripts dist/linux-amd64-cuda/
          cp -r data dist/linux-amd64-cuda/
          cp Makefile dist/linux-amd64-cuda/
          cp README.md dist/linux-amd64-cuda/
          cp LICENSE dist/linux-amd64-cuda/
          cd dist && tar -czf inference-service-linux-amd64-cuda.tar.gz linux-amd64-cuda/
          
      - name: Upload Linux CUDA artifact
        uses: actions/upload-artifact@v4
        with:
          name: linux-amd64-cuda
          path: dist/inference-service-linux-amd64-cuda.tar.gz

  build-linux-rocm:
    name: Build Linux AMD64 (ROCm/AMD GPU)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Install ROCm
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake
          # Add ROCm repository
          wget -q -O - https://repo.radeon.com/rocm/rocm.gpg.key | sudo apt-key add -
          echo 'deb [arch=amd64] https://repo.radeon.com/rocm/apt/5.7/ ubuntu main' | sudo tee /etc/apt/sources.list.d/rocm.list
          sudo apt-get update
          sudo apt-get install -y rocm-dev hip-dev || true  # May fail in CI, continue anyway
          
      - name: Build llama.cpp with ROCm
        run: |
          make build-llama-rocm || make build-llama-cpu  # Fallback to CPU if ROCm fails
          
      - name: Build inference service
        run: |
          make build-all
          
      - name: Test build
        run: |
          ./bin/inference-server --help || true
          ./bin/nats-chat --help || true
          
      - name: Package Linux ROCm release
        run: |
          mkdir -p dist/linux-amd64-rocm
          cp bin/inference-server dist/linux-amd64-rocm/
          cp bin/nats-chat dist/linux-amd64-rocm/
          cp -r envs dist/linux-amd64-rocm/
          cp -r examples dist/linux-amd64-rocm/
          cp -r scripts dist/linux-amd64-rocm/
          cp -r data dist/linux-amd64-rocm/
          cp Makefile dist/linux-amd64-rocm/
          cp README.md dist/linux-amd64-rocm/
          cp LICENSE dist/linux-amd64-rocm/
          cd dist && tar -czf inference-service-linux-amd64-rocm.tar.gz linux-amd64-rocm/
          
      - name: Upload Linux ROCm artifact
        uses: actions/upload-artifact@v4
        with:
          name: linux-amd64-rocm
          path: dist/inference-service-linux-amd64-rocm.tar.gz

  create-release:
    name: Create GitHub Release
    needs: [build-macos-metal, build-linux-amd64, build-linux-cuda, build-linux-rocm]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/')
    steps:
      - uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/

      - name: Display structure of downloaded files
        run: ls -la artifacts/

      - name: Create Release
        uses: softprops/action-gh-release@v2
        with:
          name: Release ${{ github.ref_name }}
          draft: false
          prerelease: false
          generate_release_notes: true
          files: |
            artifacts/macos-arm64-metal/inference-service-macos-arm64-metal.zip
            artifacts/linux-amd64-cpu/inference-service-linux-amd64-cpu.tar.gz
            artifacts/linux-amd64-cuda/inference-service-linux-amd64-cuda.tar.gz
            artifacts/linux-amd64-rocm/inference-service-linux-amd64-rocm.tar.gz
          body: |
            ## AI Inference Service ${{ github.ref_name }}
            
            High-performance Go microservice for AI inference with parallel model execution.
            
            ### ðŸ“¦ Downloads
            
            Choose the appropriate build for your platform:
            
            - **macOS (Apple Silicon)**: `inference-service-macos-arm64-metal.zip` - Optimized for M1/M2/M3 with Metal GPU acceleration
            - **Linux (CPU)**: `inference-service-linux-amd64-cpu.tar.gz` - CPU-only build for broad compatibility  
            - **Linux (NVIDIA GPU)**: `inference-service-linux-amd64-cuda.tar.gz` - CUDA acceleration for NVIDIA GPUs
            - **Linux (AMD GPU)**: `inference-service-linux-amd64-rocm.tar.gz` - ROCm acceleration for AMD GPUs
            
            ### ðŸš€ Quick Start
            
            1. Download and extract the appropriate build
            2. Run: `make start WORKER=gemma3-270m` (auto-downloads model)
            3. Test: `curl http://localhost:5770/healthz`
            
            ### âœ¨ Features
            
            - **Parallel Model Execution**: Multiple models loaded simultaneously
            - **Dual Access**: HTTP REST API + NATS messaging
            - **Auto-Download**: Models download automatically on first use
            - **Grammar Constraints**: GBNF grammar support for structured output
            - **Complete Observability**: SQLite logging for requests and events
            - **Production Ready**: Clean architecture, structured logging, graceful shutdown
            
            Built by Sistemica GmbH for production AI workloads.
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}