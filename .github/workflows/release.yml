name: Create Release

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:

jobs:
  create-release:
    name: Create GitHub Release
    runs-on: ubuntu-22.04
    if: startsWith(github.ref, 'refs/tags/')
    steps:
      - uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/

      - name: Display structure of downloaded files
        run: ls -la artifacts/

      - name: Create Release
        uses: softprops/action-gh-release@v2
        with:
          name: Release ${{ github.ref_name }}
          draft: false
          prerelease: false
          generate_release_notes: true
          files: |
            artifacts/macos-arm64-metal/inference-service-macos-arm64-metal.zip
            artifacts/linux-amd64-cpu/inference-service-linux-amd64-cpu.tar.gz
            artifacts/linux-amd64-cuda/inference-service-linux-amd64-cuda.tar.gz
            artifacts/linux-amd64-rocm/inference-service-linux-amd64-rocm.tar.gz
          body: |
            ## AI Inference Service ${{ github.ref_name }}
            
            High-performance Go microservice for AI inference with parallel model execution.
            
            ### ðŸ“¦ Downloads
            
            Choose the appropriate build for your platform:
            
            - **macOS (Apple Silicon)**: `inference-service-macos-arm64-metal.zip` - Optimized for M1/M2/M3 with Metal GPU acceleration
            - **Linux (CPU)**: `inference-service-linux-amd64-cpu.tar.gz` - CPU-only build for broad compatibility  
            - **Linux (NVIDIA GPU)**: `inference-service-linux-amd64-cuda.tar.gz` - CUDA acceleration for NVIDIA GPUs
            - **Linux (AMD GPU)**: `inference-service-linux-amd64-rocm.tar.gz` - ROCm acceleration for AMD GPUs
            
            ### ðŸš€ Quick Start
            
            1. Download and extract the appropriate build
            2. Run: `make start WORKER=gemma3-270m` (auto-downloads model)
            3. Test: `curl http://localhost:5770/healthz`
            
            ### âœ¨ Features
            
            - **Parallel Model Execution**: Multiple models loaded simultaneously
            - **Dual Access**: HTTP REST API + NATS messaging
            - **Auto-Download**: Models download automatically on first use
            - **Grammar Constraints**: GBNF grammar support for structured output
            - **Complete Observability**: SQLite logging for requests and events
            - **Production Ready**: Clean architecture, structured logging, graceful shutdown
            
            Built by Sistemica GmbH for production AI workloads.
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}